{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import MessagePassing, global_max_pool\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'train.parquet'\n",
    "test_path = 'test.parquet'\n",
    "\n",
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsa_df = con.query(f\"\"\"(SELECT *\n",
    "                        FROM parquet_scan('{train_path}')\n",
    "                        WHERE protein_name = 'HSA' AND binds = 1)\n",
    "                        UNION ALL\n",
    "                        (SELECT *\n",
    "                        FROM parquet_scan('{train_path}')\n",
    "                        WHERE protein_name = 'HSA' AND binds = 0\n",
    "                        ORDER BY random()\n",
    "                        LIMIT 408410)\n",
    "                        \"\"\").df()\n",
    "\n",
    "brd4_df = con.query(f\"\"\"(SELECT *\n",
    "                        FROM parquet_scan('{train_path}')\n",
    "                        WHERE protein_name = 'BRD4' AND binds = 1)\n",
    "                        UNION ALL\n",
    "                        (SELECT *\n",
    "                        FROM parquet_scan('{train_path}')\n",
    "                        WHERE protein_name = 'BRD4' AND binds = 0\n",
    "                        ORDER BY random()\n",
    "                        LIMIT 456964)\n",
    "                        \"\"\").df()\n",
    "\n",
    "seh_df = con.query(f\"\"\"(SELECT *\n",
    "                        FROM parquet_scan('{train_path}')\n",
    "                        WHERE protein_name = 'sEH' AND binds = 1)\n",
    "                        UNION ALL\n",
    "                        (SELECT *\n",
    "                        FROM parquet_scan('{train_path}')\n",
    "                        WHERE protein_name = 'sEH' AND binds = 0\n",
    "                        ORDER BY random()\n",
    "                        LIMIT 724532)\n",
    "                        \"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binds\n",
      "1    408410\n",
      "0    408410\n",
      "Name: count, dtype: int64\n",
      "binds\n",
      "1    456964\n",
      "0    456964\n",
      "Name: count, dtype: int64\n",
      "binds\n",
      "1    724532\n",
      "0    724532\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(hsa_df['binds'].value_counts())\n",
    "print(brd4_df['binds'].value_counts())\n",
    "print(seh_df['binds'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hsa, test_hsa = train_test_split(hsa_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "train_brd4, test_brd4 = train_test_split(brd4_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "train_seh, test_seh = train_test_split(seh_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "#remember to separate the labels from the test dfs after featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hsa_x = test_hsa.drop(columns= ['binds'])\n",
    "test_hsa_y = test_hsa['binds']\n",
    "\n",
    "test_brd4_x = test_brd4.drop(columns= ['binds'])\n",
    "test_brd4_y = test_brd4['binds']\n",
    "\n",
    "test_seh_x = test_seh.drop(columns= ['binds'])\n",
    "test_seh_y = test_seh['binds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsa_data = {\n",
    "    'train': train_hsa,\n",
    "    'test_x': test_hsa_x,\n",
    "}\n",
    "\n",
    "brd4_data = {\n",
    "    'train': train_brd4,\n",
    "    'test_x': test_brd4_x,\n",
    "}\n",
    "\n",
    "seh_data = {\n",
    "    'train': train_seh,\n",
    "    'test_x': test_seh_x,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_data_object(smiles, ids, labels=None):\n",
    "\n",
    "    def _one_hot_encoding(element, permitted_elements):\n",
    "        \"\"\"\n",
    "        Maps input elements element which are not in the permitted list to the last element of the permitted list\n",
    "        \"\"\"\n",
    "        if element not in permitted_elements:\n",
    "            element = permitted_elements[-1]\n",
    "\n",
    "        binary_encoding = [int(boolean_value) for boolean_value in list(map(lambda s: element==s , permitted_elements))]\n",
    "\n",
    "        return binary_encoding\n",
    "\n",
    "\n",
    "    def _get_atom_features(atom):\n",
    "        \n",
    "        #Define a simplified list of atom types\n",
    "        permitted_atom_types = [\n",
    "            'C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg',\n",
    "            'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl',\n",
    "            'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H',\n",
    "            'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n",
    "            'Pt', 'Hg', 'Pb', 'Dy', 'Unknown'\n",
    "        ]\n",
    "        atom_type = atom.GetSymbol() if atom.GetSymbol() in permitted_atom_types else 'Unknown'\n",
    "        atom_type_enc = _one_hot_encoding(atom_type, permitted_atom_types)\n",
    "\n",
    "        hybridization_type = [\n",
    "            Chem.rdchem.HybridizationType.S,\n",
    "            Chem.rdchem.HybridizationType.SP,\n",
    "            Chem.rdchem.HybridizationType.SP2,\n",
    "            Chem.rdchem.HybridizationType.SP3,\n",
    "            Chem.rdchem.HybridizationType.SP3D\n",
    "        ]\n",
    "        atom_hybridization_type = _one_hot_encoding(atom.GetHybridization(), hybridization_type)\n",
    "\n",
    "        atom_degree = _one_hot_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5])\n",
    "\n",
    "        is_in_ring = [int(atom.IsInRing())]\n",
    "\n",
    "        total_hs = _one_hot_encoding(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5])\n",
    "\n",
    "        implicit_valence = _one_hot_encoding(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5])\n",
    "\n",
    "        chirality = _one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
    "        \n",
    "        atom_features = atom_type_enc + atom_degree + is_in_ring + total_hs + implicit_valence + atom_hybridization_type + chirality\n",
    "        \n",
    "        return np.array(atom_features, dtype=np.float32)\n",
    "\n",
    "\n",
    "    def _get_bond_features(bond):\n",
    "\n",
    "        bond_type = bond.GetBondType()\n",
    "\n",
    "        features = [\n",
    "            int(bond_type == Chem.rdchem.BondType.SINGLE),\n",
    "            int(bond_type == Chem.rdchem.BondType.DOUBLE),\n",
    "            int(bond_type == Chem.rdchem.BondType.TRIPLE),\n",
    "            int(bond_type == Chem.rdchem.BondType.AROMATIC),\n",
    "            int(bond.IsInRing()),\n",
    "            int(bond.GetIsConjugated()),\n",
    "        ]\n",
    "        \n",
    "        return np.array(features, dtype=np.float32)\n",
    "    \n",
    "\n",
    "    data_list = []\n",
    "    \n",
    "    for index, smile in enumerate(smiles):\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        \n",
    "        if not mol:  # Skip invalid SMILES strings\n",
    "            continue\n",
    "        \n",
    "        # Node features\n",
    "        atom_features = [_get_atom_features(atom) for atom in mol.GetAtoms()]\n",
    "        x = torch.tensor(atom_features, dtype=torch.float)\n",
    "        \n",
    "        # Edge features\n",
    "        edge_index = []\n",
    "        edge_features = []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            edge_index += [(start, end), (end, start)]  # Undirected graph\n",
    "            bond_feature = _get_bond_features(bond)\n",
    "            edge_features += [bond_feature, bond_feature]  # Same features in both directions\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "        \n",
    "        # Creating the Data object\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        data.molecule_id = ids[index]\n",
    "\n",
    "        if labels is not None:\n",
    "            data.y = torch.tensor([labels[index]], dtype=torch.float)\n",
    "        \n",
    "        data_list.append(data)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "\n",
    "def featurize_data_in_batches(ids_array, smiles_array, labels_array, batch_size = 2**10):\n",
    "    data_list = []\n",
    "    # Define tqdm progress bar\n",
    "    pbar = tqdm(total=len(smiles_array), desc=\"Featurizing data\")\n",
    "    for i in range(0, len(smiles_array), batch_size):\n",
    "        smiles_batch = smiles_array[i:i+batch_size]\n",
    "        ids_batch = ids_array[i:i+batch_size]\n",
    "        labels_batch = labels_array[i:i+batch_size] if labels_array is not None else None\n",
    "        batch_data_list = get_torch_data_object(smiles_batch, ids_batch, labels_batch)\n",
    "        data_list.extend(batch_data_list)\n",
    "        pbar.update(len(smiles_batch))\n",
    "        \n",
    "    pbar.close()\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def get_featurized_data(protein_data: dict):\n",
    "\n",
    "    featurized_data = {}\n",
    "\n",
    "    for key in protein_data.keys():\n",
    "        if 'molecule_smiles' in protein_data[key]:\n",
    "            smiles_array = np.array(protein_data[key]['molecule_smiles'])\n",
    "            ids_array = np.array(protein_data[key]['id'])\n",
    "\n",
    "            labels_array = None\n",
    "            \n",
    "            if 'binds' in protein_data[key]:\n",
    "                labels_array = np.array(protein_data[key]['binds'])\n",
    "        \n",
    "        featurized_data[key] = featurize_data_in_batches(ids_array, smiles_array, labels_array)\n",
    "        \n",
    "    return featurized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing data:   0%|          | 0/653456 [00:00<?, ?it/s]/var/folders/gk/_xtwg0dn10q11r8rrsypc75m0000gn/T/ipykernel_85021/2784521092.py:78: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  x = torch.tensor(atom_features, dtype=torch.float)\n",
      "Featurizing data: 100%|██████████| 653456/653456 [14:02<00:00, 775.79it/s]\n",
      "Featurizing data: 100%|██████████| 163364/163364 [03:28<00:00, 783.43it/s]\n",
      "Featurizing data: 100%|██████████| 731142/731142 [15:50<00:00, 768.93it/s]\n",
      "Featurizing data: 100%|██████████| 182786/182786 [03:59<00:00, 762.37it/s]\n",
      "Featurizing data: 100%|██████████| 1159251/1159251 [24:50<00:00, 778.02it/s]\n",
      "Featurizing data: 100%|██████████| 289813/289813 [06:05<00:00, 793.66it/s]\n"
     ]
    }
   ],
   "source": [
    "hsa_featurized_data = get_featurized_data(hsa_data)\n",
    "brd4_featurized_data = get_featurized_data(brd4_data)\n",
    "seh_featurized_data = get_featurized_data(seh_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(hsa_featurized_data, 'hsa_featurized_data')\n",
    "torch.save(brd4_featurized_data, 'brd4_featurized_data')\n",
    "torch.save(seh_featurized_data, 'seh_featurized_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGNNLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CustomGNNLayer, self).__init__(aggr='max')\n",
    "        self.lin = nn.Linear(in_channels + 6, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Start propagating messages\n",
    "        return MessagePassing.propagate(self, edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        combined = torch.cat((x_j, edge_attr), dim=1)\n",
    "        return combined\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return self.lin(aggr_out)\n",
    "\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_rate):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList([CustomGNNLayer(input_dim if i == 0 else hidden_dim, hidden_dim) for i in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers)])\n",
    "        self.lin = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "\n",
    "        x = global_max_pool(x, data.batch) # Global pooling to get a graph-level representation\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        loader,\n",
    "        num_epochs,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        num_layers,\n",
    "        dropout_rate,\n",
    "        lr,\n",
    "        save_path\n",
    "    ):\n",
    "    model = GNNModel(input_dim, hidden_dim, num_layers, dropout_rate)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch.y.view(-1,1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(loader)}')\n",
    "    \n",
    "    torch.save(model, save_path)\n",
    "\n",
    "def predict_with_model(model, test_loader):\n",
    "    #model.eval()\n",
    "    predictions = []\n",
    "    molecule_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            output = torch.sigmoid(model(data))\n",
    "            predictions.extend(output.view(-1).tolist())\n",
    "            molecule_ids.extend(data.molecule_id)\n",
    "    \n",
    "    return molecule_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for the current protein\n",
    "hsa_train_loader = DataLoader(hsa_featurized_data['train'], batch_size=32, shuffle=True)\n",
    "hsa_test_loader = DataLoader(hsa_featurized_data['test_x'], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11, Loss: 0.41445273210845784\n",
      "Epoch 2/11, Loss: 0.37483675144753253\n",
      "Epoch 3/11, Loss: 0.3640241125902013\n",
      "Epoch 4/11, Loss: 0.35805150209816844\n",
      "Epoch 5/11, Loss: 0.354398698350935\n",
      "Epoch 6/11, Loss: 0.35250538794950337\n",
      "Epoch 7/11, Loss: 0.3508440396946824\n",
      "Epoch 8/11, Loss: 0.34916548866403363\n",
      "Epoch 9/11, Loss: 0.34850780542956866\n",
      "Epoch 10/11, Loss: 0.3480572686311512\n",
      "Epoch 11/11, Loss: 0.34701293485138723\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "input_dim = hsa_train_loader.dataset[0].num_node_features\n",
    "hidden_dim = 64\n",
    "num_epochs = 11\n",
    "num_layers = 4 #can be modified\n",
    "dropout_rate = 0.3\n",
    "lr = 0.001\n",
    "save_path = 'hsa_trained_model.pt'\n",
    "train_model(hsa_train_loader,num_epochs, input_dim, hidden_dim,num_layers, dropout_rate, lr, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gk/_xtwg0dn10q11r8rrsypc75m0000gn/T/ipykernel_85021/52822848.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('hsa_trained_model.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "__main__.GNNModel"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('hsa_trained_model.pt')\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "molecule_ids, predictions = predict_with_model(model, hsa_test_loader)\n",
    "\n",
    "# Collect predictions\n",
    "hsa_predictions = pd.DataFrame({\n",
    "    'id': molecule_ids,\n",
    "    'binds': predictions,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.75      0.84    103957\n",
      "           1       0.68      0.94      0.79     59407\n",
      "\n",
      "    accuracy                           0.82    163364\n",
      "   macro avg       0.82      0.84      0.81    163364\n",
      "weighted avg       0.85      0.82      0.82    163364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate predictions \n",
    "BINDING_THRESHOLD = 0.8\n",
    "testhsax = hsa_predictions['binds'].apply(lambda x: 1 if x >= BINDING_THRESHOLD else 0)\n",
    "\n",
    "print(classification_report(testhsax, test_hsa_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.996935\n",
       "1         0.484345\n",
       "2         0.917875\n",
       "3         0.780834\n",
       "4         0.988841\n",
       "            ...   \n",
       "163359    0.552897\n",
       "163360    0.774370\n",
       "163361    0.936678\n",
       "163362    0.976615\n",
       "163363    0.318769\n",
       "Name: binds, Length: 163364, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsa_predictions['binds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.003\u001b[39m\n\u001b[1;32m      8\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhsa_trained_model_2.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhsa_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(loader, num_epochs, input_dim, hidden_dim, num_layers, dropout_rate, lr, save_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, batch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     22\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Repositories/small-molecule-protein-binding-affinity/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repositories/small-molecule-protein-binding-affinity/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m, in \u001b[0;36mGNNModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     28\u001b[0m x, edge_index, edge_attr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39medge_attr\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbns[i](x)\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/Repositories/small-molecule-protein-binding-affinity/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repositories/small-molecule-protein-binding-affinity/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m, in \u001b[0;36mCustomGNNLayer.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Start propagating messages\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMessagePassing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repositories/small-molecule-protein-binding-affinity/.venv/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:548\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    543\u001b[0m         kwargs[arg] \u001b[38;5;241m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[1;32m    545\u001b[0m coll_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_args, edge_index,\n\u001b[1;32m    546\u001b[0m                           mutable_size, kwargs)\n\u001b[0;32m--> 548\u001b[0m msg_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minspector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_param_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoll_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    551\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (msg_kwargs, ))\n",
      "File \u001b[0;32m~/Repositories/small-molecule-protein-binding-affinity/.venv/lib/python3.12/site-packages/torch_geometric/inspector.py:299\u001b[0m, in \u001b[0;36mInspector.collect_param_data\u001b[0;34m(self, func, kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the union of parameter names of all inspected functions in\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    :obj:`funcs`.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m            (default: :obj:`None`)\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_flat_param_dict(funcs, exclude)\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect_param_data\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    301\u001b[0m     func: Union[Callable, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    302\u001b[0m     kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    303\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Collects the input data of the inspected function :obj:`func`\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    according to its function signature from a data blob.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m        kwargs (dict[str, Any]): The data blob which may serve as inputs.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     out_dict: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "input_dim = hsa_train_loader.dataset[0].num_node_features\n",
    "hidden_dim = 64\n",
    "num_epochs = 11\n",
    "num_layers = 4 #can be modified\n",
    "dropout_rate = 0.3\n",
    "lr = 0.003\n",
    "save_path = 'hsa_trained_model_2.pt'\n",
    "train_model(hsa_train_loader,num_epochs, input_dim, hidden_dim,num_layers, dropout_rate, lr, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1z/n81k2kv92mz8zcnldchy94fh0000gp/T/ipykernel_58690/3358583233.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_2 = torch.load('hsa_trained_model_2.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.72      0.82    109244\n",
      "           1       0.63      0.94      0.75     54120\n",
      "\n",
      "    accuracy                           0.79    163364\n",
      "   macro avg       0.79      0.83      0.79    163364\n",
      "weighted avg       0.85      0.79      0.80    163364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2 = torch.load('hsa_trained_model_2.pt')\n",
    "# Predict\n",
    "molecule_ids, predictions = predict_with_model(model, hsa_test_loader)\n",
    "\n",
    "# Collect predictions\n",
    "hsa_predictions = pd.DataFrame({\n",
    "    'id': molecule_ids,\n",
    "    'binds': predictions,\n",
    "})\n",
    "\n",
    "#Evaluate predictions \n",
    "BINDING_THRESHOLD = 0.8\n",
    "hsa_predictions['binds'] = hsa_predictions['binds'].apply(lambda x: 1 if x > BINDING_THRESHOLD else 0)\n",
    "\n",
    "print(classification_report(hsa_predictions['binds'], test_hsa_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for the current protein\n",
    "brd4_train_loader = DataLoader(brd4_featurized_data['train'], batch_size=32, shuffle=True)\n",
    "brd4_test_loader = DataLoader(brd4_featurized_data['test_x'], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11, Loss: 0.3241531908028866\n",
      "Epoch 2/11, Loss: 0.2774880223373698\n",
      "Epoch 3/11, Loss: 0.2658897035027368\n",
      "Epoch 4/11, Loss: 0.259332908411439\n",
      "Epoch 5/11, Loss: 0.25706945616999566\n",
      "Epoch 6/11, Loss: 0.2554396822979463\n",
      "Epoch 7/11, Loss: 0.25418353400633076\n",
      "Epoch 8/11, Loss: 0.2530095343909223\n",
      "Epoch 9/11, Loss: 0.25223422029232023\n",
      "Epoch 10/11, Loss: 0.2517589669030792\n",
      "Epoch 11/11, Loss: 0.25102477654184735\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "input_dim = brd4_train_loader.dataset[0].num_node_features\n",
    "hidden_dim = 64\n",
    "num_epochs = 11\n",
    "num_layers = 4 #can be modified\n",
    "dropout_rate = 0.3\n",
    "lr = 0.001\n",
    "save_path = 'brd4_trained_model_2.pt'\n",
    "train_model(brd4_train_loader,num_epochs, input_dim, hidden_dim,num_layers, dropout_rate, lr, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gk/_xtwg0dn10q11r8rrsypc75m0000gn/T/ipykernel_85021/1787329386.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_6 = torch.load('brd4_trained_model_2.pt')\n"
     ]
    }
   ],
   "source": [
    "model_6 = torch.load('brd4_trained_model_2.pt')\n",
    "# Predict\n",
    "molecule_ids, predictions = predict_with_model(model_6, brd4_test_loader)\n",
    "\n",
    "# Collect predictions\n",
    "brd4_predictions = pd.DataFrame({\n",
    "    'id': molecule_ids,\n",
    "    'binds': predictions,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89    104358\n",
      "           1       0.81      0.95      0.87     78428\n",
      "\n",
      "    accuracy                           0.88    182786\n",
      "   macro avg       0.88      0.89      0.88    182786\n",
      "weighted avg       0.89      0.88      0.88    182786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate predictions \n",
    "BINDING_THRESHOLD = 0.7\n",
    "brd4_predictions_binds = brd4_predictions['binds'].apply(lambda x: 1 if x > BINDING_THRESHOLD else 0)\n",
    "\n",
    "print(classification_report(brd4_predictions_binds, test_brd4_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for the current protein\n",
    "seh_train_loader = DataLoader(seh_featurized_data['train'], batch_size=32, shuffle=True)\n",
    "seh_test_loader = DataLoader(seh_featurized_data['test_x'], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11, Loss: 0.19211339599879126\n",
      "Epoch 2/11, Loss: 0.16134811337898122\n",
      "Epoch 3/11, Loss: 0.15541458889830928\n",
      "Epoch 4/11, Loss: 0.15288605839218689\n",
      "Epoch 5/11, Loss: 0.15138296992422245\n",
      "Epoch 6/11, Loss: 0.15018682266923408\n",
      "Epoch 7/11, Loss: 0.1487006692093019\n",
      "Epoch 8/11, Loss: 0.14760054968567723\n",
      "Epoch 9/11, Loss: 0.14777189984626987\n",
      "Epoch 10/11, Loss: 0.14661013126019756\n",
      "Epoch 11/11, Loss: 0.146275563309343\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "input_dim = seh_train_loader.dataset[0].num_node_features\n",
    "hidden_dim = 64\n",
    "num_epochs = 11\n",
    "num_layers = 4 #can be modified\n",
    "dropout_rate = 0.3\n",
    "lr = 0.001\n",
    "save_path = 'seh_trained_model.pt'\n",
    "train_model(seh_train_loader,num_epochs, input_dim, hidden_dim,num_layers, dropout_rate, lr, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gk/_xtwg0dn10q11r8rrsypc75m0000gn/T/ipykernel_85021/3561590640.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_5 = torch.load('seh_trained_model.pt')\n"
     ]
    }
   ],
   "source": [
    "model_5 = torch.load('seh_trained_model.pt')\n",
    "# Predict\n",
    "molecule_ids, predictions = predict_with_model(model_5, seh_test_loader)\n",
    "\n",
    "# Collect predictions\n",
    "seh_predictions = pd.DataFrame({\n",
    "    'id': molecule_ids,\n",
    "    'binds': predictions,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95    149851\n",
      "           1       0.93      0.96      0.94    139962\n",
      "\n",
      "    accuracy                           0.94    289813\n",
      "   macro avg       0.94      0.94      0.94    289813\n",
      "weighted avg       0.94      0.94      0.94    289813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate predictions \n",
    "BINDING_THRESHOLD = 0.6\n",
    "seh_predictions_binds = seh_predictions['binds'].apply(lambda x: 1 if x > BINDING_THRESHOLD else 0)\n",
    "\n",
    "print(classification_report(seh_predictions_binds, test_seh_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
